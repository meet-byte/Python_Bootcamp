{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "Optimizing Pandas performance is essential when working with large datasets to avoid memory inefficiencies and slow computations. Here are several strategies that can be used to speed up your Pandas operations and optimize performance:\n",
    "\n",
    "1. Use Vectorized Operations (Avoid Loops)\n",
    "Vectorized operations are much faster than iterating through data manually using loops. Pandas and NumPy are optimized for vectorized operations, meaning you should leverage them whenever possible. This means applying operations to whole columns or DataFrames at once.\n",
    "Example:\n",
    "Instead of using a loop to add 10 to each value in a column:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "df['A'] = df['A'] + 10  # Vectorized operation\n",
    "Avoid:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "for i in range(len(df)):\n",
    "    df['A'][i] += 10  # Slow loop-based operation\n",
    "2. Use apply() and applymap() Carefully\n",
    "apply() is used to apply a function along a particular axis (rows or columns), but it can be slower than vectorized operations. If possible, try to use built-in vectorized operations like .sum(), .mean(), etc., instead of apply().\n",
    "\n",
    "Use applymap() only for element-wise operations (when needed) on DataFrames, but avoid it on Series because it can be slow.\n",
    "\n",
    "Example:\n",
    "Instead of:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "df['A'] = df['A'].apply(lambda x: x + 10)\n",
    "Use:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "df['A'] += 10  # More efficient\n",
    "3. Use Efficient Data Types\n",
    "Choosing the correct data type (dtype) for your columns can significantly reduce memory usage and improve performance. For example, use int8 or float32 instead of the default int64 or float64 if your data range allows it.\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "df['int_column'] = df['int_column'].astype('int32')  # Reduces memory\n",
    "df['category_column'] = df['category_column'].astype('category')  # Optimizes string columns\n",
    "4. Filter Data Early (Reduce Data Size)\n",
    "Filter the data as much as possible before performing operations to reduce the amount of data that needs to be processed.\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "df_filtered = df[df['column'] > 50]  # Filter out unnecessary data early\n",
    "5. Use category dtype for Categorical Data\n",
    "If you have columns with repeated string values, converting them to the category dtype can save memory and speed up computations.\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "df['category_column'] = df['category_column'].astype('category')\n",
    "This is especially useful for categorical variables with many repeated entries, like gender, city, or product type.\n",
    "\n",
    "6. Use inplace=True When Modifying DataFrames\n",
    "Using the inplace=True parameter can reduce memory usage because it avoids creating copies of the DataFrame during operations.\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "df.dropna(inplace=True)  # Avoids creating a copy of the DataFrame\n",
    "7. Use numba or Cython for Custom Functions\n",
    "Numba and Cython are tools that allow you to write Python code that runs faster by compiling it into machine code.\n",
    "\n",
    "Numba: Used for numeric operations that can be executed with high performance.\n",
    "\n",
    "Cython: Can be used for both numeric and non-numeric operations, and allows you to write Python code that behaves like C code.\n",
    "\n",
    "Example with Numba:\n",
    "python\n",
    "Copy code\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def slow_function(x):\n",
    "    # Some complex operation\n",
    "    return x * 2\n",
    "\n",
    "df['result'] = df['column'].apply(slow_function)\n",
    "8. Optimize groupby() Operations\n",
    "groupby() operations can be slow, so you can optimize them by:\n",
    "Aggregating in a single step rather than applying multiple aggregation functions.\n",
    "Using sort=False when you don't need to sort the groups.\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "# Instead of this:\n",
    "grouped = df.groupby('category')['value'].mean().reset_index()\n",
    "\n",
    "# Do this:\n",
    "grouped = df.groupby('category', sort=False)['value'].mean().reset_index()\n",
    "9. Use dask or modin for Larger-than-memory Datasets\n",
    "If you're dealing with very large datasets that do not fit in memory, you can use Dask or Modin:\n",
    "Dask: Parallelizes operations and scales from a single machine to a cluster.\n",
    "Modin: Provides a drop-in replacement for Pandas with automatic parallelization on multiple cores or machines.\n",
    "Example using Dask:\n",
    "python\n",
    "Copy code\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('large_file.csv')\n",
    "df = df[df['column'] > 50]  # Filtering is done in parallel\n",
    "10. Use merge() Efficiently\n",
    "When performing merges or joins, always try to join on indexed columns (i.e., the column you join on should be set as an index) to speed up the operation.\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "df.set_index('key_column', inplace=True)\n",
    "other_df.set_index('key_column', inplace=True)\n",
    "\n",
    "# Now perform the merge\n",
    "result = df.merge(other_df, left_index=True, right_index=True)\n",
    "11. Use HDF5 or Feather for Efficient Storage and I/O\n",
    "For saving and loading large datasets, formats like HDF5 and Feather are much faster than CSV and Excel. They also consume less memory.\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "df.to_hdf('data.h5', key='df', mode='w')  # Save DataFrame in HDF5 format\n",
    "df = pd.read_hdf('data.h5', key='df')  # Load DataFrame from HDF5 format\n",
    "12. Use query() for Efficient Filtering\n",
    "query() is a fast way to filter data using an expression string. It avoids the overhead of constructing a condition with & and |.\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "df.query('age > 30 and salary > 50000', inplace=True)\n",
    "13. Avoid Chained Indexing\n",
    "Chained indexing (accessing a DataFrame in multiple steps) can be slower and lead to unexpected behavior. Instead, use direct assignments.\n",
    "Bad Example (Chained Indexing):\n",
    "python\n",
    "Copy code\n",
    "df[df['column'] > 50]['other_column'] = 100  # Chained indexing is slower\n",
    "Good Example (Direct Assignment):\n",
    "python\n",
    "Copy code\n",
    "df.loc[df['column'] > 50, 'other_column'] = 100  # More efficient\n",
    "Summary of Optimization Tips:\n",
    "Use vectorized operations instead of loops.\n",
    "Choose efficient data types (e.g., category for strings, int32 for integers).\n",
    "Filter data early to reduce the size of the dataset.\n",
    "Use apply() carefully and prefer built-in vectorized functions.\n",
    "Use Dask or Modin for handling large datasets that donâ€™t fit in memory.\n",
    "Store and load data using HDF5 or Feather for faster I/O.\n",
    "Optimize groupby() operations.\n",
    "Minimize memory usage by using inplace operations where possible.\n",
    "Avoid chained indexing for faster and cleaner code.\n",
    "Use query() for filtering and operations on DataFrames.\n",
    "By following these strategies, you can significantly improve the performance and efficiency of your Pandas workflows, especially when dealing with large datasets.\n",
    "\n",
    "Let me know if you need more examples or further clarification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'large_excel_file.xlsx'\n",
    "\n",
    "dtypes = {\n",
    "    'Column1': 'int64',\n",
    "    'Column2': 'float64',\n",
    "    'Column3': 'object',\n",
    "}\n",
    "\n",
    "usecols = ['Column1', 'Column2', 'Column3']\n",
    "\n",
    "df = pd.read_excel(file_path, dtype=dtypes, usecols=usecols)\n",
    "\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
